{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY4pDTYWQSnh"
      },
      "source": [
        "<h3>importing </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OKZJDRJyy4WJ"
      },
      "outputs": [],
      "source": [
        "#uncomment below line to install libraries needed for this project\n",
        "\n",
        "# !pip install hazm gdown numpy pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "bPnDVVBjHms4",
        "outputId": "5fe51265-064e-430d-886e-e3d408eece66"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hazm as hz\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9j9XPUM-dk9"
      },
      "source": [
        "<h3>downloading files and dependacies</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6X3WMMstzKrs"
      },
      "outputs": [],
      "source": [
        "#uncomment the lines below to dowmload the files needed for this project\n",
        "\n",
        "# gdown.download(id = \"1E7HUmSArVP6LmyaOjByka2dYME_iBS47\",output=\"books_train.csv\")\n",
        "# gdown.download(id = \"1nB61mWEy1vYcZyyPwiifQBwz_Hgo1EJc\",output=\"books_test.csv\")\n",
        "# gdown.download(id = \"1D40HFl-gClrxaD606SVag4dfA_OD9Qlt\",output=\"spec_chars.txt\")\n",
        "# gdown.download(id = \"17xQuS6Y0xgT80aGcTjLrR8CpUGN85QsR\",output=\"words_to_delete.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-22wGA3DJH72"
      },
      "source": [
        "<h3>reading the data frames</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">اولین قدم در اجرای این پروژه، خواندن فایل های تست و ترین با کمک کتابخانه پانداز می‌باشد</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2jEi1vrWJPg6"
      },
      "outputs": [],
      "source": [
        "books_test = pd.read_csv(\"books_test.csv\")\n",
        "\n",
        "books_train = pd.read_csv(\"books_train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjT8_JbWQC8p"
      },
      "source": [
        "<h3>phase 1: preprocessing</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در مرحله بعد، دو فایل با کمک پانداز بارگذاری میشود. فایل اول، شامل حروف اضافه، حروف ربط، ضمایر و برخی صفات اشاره است. فایل دوم شامل علائم نگارشی، اعداد و برخی از یونیکد های خاص فارسی است. علت تغییر آرکومان های delimiter و qutochar، واکنش خاصی است که پانداز به علائم کاما و دابل کوتیشن نشان میدهد</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xmLAApyVC4ak"
      },
      "outputs": [],
      "source": [
        "words_to_delete = list(pd.read_csv(\n",
        "    \"words_to_delete.txt\"\n",
        "    ,header=None\n",
        "    ,names=[\"dels\"]\n",
        "    )[\"dels\"])\n",
        "spec_chars = list(pd.read_csv(\n",
        "    \"spec_chars.txt\",\n",
        "    header=None,\n",
        "    names=['chr'],\n",
        "    delimiter=\" \",\n",
        "    quotechar= \" \"\n",
        "    )['chr'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">دو آبجکت نرمالایزر و توکنایزر برای استفاده از متد های این دو کلاس از کابخانه هضم ساخته میشود.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L7Oi7nU4_w0x"
      },
      "outputs": [],
      "source": [
        "norm_obj = hz.Normalizer()\n",
        "token_obj =  hz.WordTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">برویم سراغ اولین حرکت جدی این پروژه! تابع کلین تکست، وظیفه انجام اکشن های پری پراسس را بر روی استرینگ ورودی به این تابع دارد. در ابتدا از متد نرمالایز هضم استفاده میشود. این متد تا حدود خوبی متن را شبیه به متن نوشتاری رسمی میکند، با انجام کارهایی مانند دور ریختن اسپیس های تکراری،از بین بردن اعراب حروف، تبدیل اعداد به اعداد فارسی، درست کردن نیم فاصله افعال و ... که در روند پردازش متن کمک شایانی میکند. سپس از طربق متد replace نیم فاصله را با فاصله جایگذاری میکنیم تا پیشوند ها و پسوند های اضافی که پیشتر بار گذاری کرده بودیم را حذف کنیم. در مرحله بعد، با استفاده از متد توکنایز کتابخانه هضم، کلمات را از هم جدا میکنیم و در قالب لیست نگه میداریم. متد توکنایز با بررسی جمله، کلمات، اعداد و علائم را از هم جدا کرده و در قالب لیست بر میگرداند. حال کلماتی را که در لیست <<کلمات قابل حذف>> نیستند را شناسایی کرده و به هم میچسبانیم. در مرحله بعد، روی استرینگ ایجاد شده حرکت میکنیم و هر حرفی که جزو حروف فایل علائم نبود را انتخاب و دوباره به هم میچسبانیم. در مرحله آخر، متن تمیز شده را بر میگردانیم. به کد این بخش توجه کنید: </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "647LLxTX-qQH"
      },
      "outputs": [],
      "source": [
        "def clean_text(text : str) -> str:\n",
        "  text = norm_obj.normalize(text).replace(\"‌\",\" \")\n",
        "  tokenized_text_list = token_obj.tokenize(text)\n",
        "  semi_final_text = \" \".join([x for x in tokenized_text_list if x not in words_to_delete])\n",
        "  final_text = \"\".join([x for x in semi_final_text if x not in spec_chars])\n",
        "  return final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\"> حال در این بخش ابتدا نگاهی کلی به دیتا فریم تست می اندازیم تا ببینیم آیا داده گمشده داریم یا خیر، که متوجه می شویم نداریم. سپس اعمال پری پراسس را روی ستون توضیحات این دیتا فریم، با کمک متد اپلای پانداز (که یک تابع را به طور همزمان روی همه سطر های یک دیتا فریم پیاده میکند) پیاده میکنیم. در مرحله آخر هم ابتدای ستون توضیحات را یک نگاه کلی می اندازیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4WwuMzwfQNDU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2550 entries, 0 to 2549\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   title        2550 non-null   object\n",
            " 1   description  2550 non-null   object\n",
            " 2   categories   2550 non-null   object\n",
            "dtypes: object(3)\n",
            "memory usage: 59.9+ KB\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0     ساختار نظریه جامعه شناسی ایران  نوشته ابوالفض...\n",
              "1     جامعه فرهنگ کانادا  مجموعه کتاب  جامعه فرهنگ ...\n",
              "2    پرسش مختلفی زندگی شخصیت امام مهدی  عج  اذهان م...\n",
              "3     موج دریا  قلم مهری ماهوتی     تصویرگری عاطفه ...\n",
              "4     پرسش غرب  قلم دکتر اسماعیل شفیعی سروستانی  نو...\n",
              "5     خارج خط  مجموعه داستان کوتاهی نوشته محمود راج...\n",
              "6     لاک صورتی  نوشته جلال آل احمد      نویسنده فع...\n",
              "7    راه بسیار زیادی سرمایه گذاری دنیا وجود دارد  ی...\n",
              "8    شکل گیری رشد روز افزون دهکده جهانی  بشر بیش زم...\n",
              "9     رویکردی جدید اختلالات مصرف مواد مشاوره اعتیاد...\n",
              "Name: description, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "books_train.info(verbose=True,show_counts=True)\n",
        "\n",
        "books_train[\"description\"] = books_train[\"description\"].apply(clean_text)\n",
        "books_train[\"description\"].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در این بخش همه اعمال بخش قبل را روی دیتا فریم تست پیاده میکنیم. نکته مهم اینکه این دیتافریم نیز مقادیر گمشده نداشت.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4lKDYflLpTVK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 450 entries, 0 to 449\n",
            "Data columns (total 3 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   title        450 non-null    object\n",
            " 1   description  450 non-null    object\n",
            " 2   categories   450 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 10.7+ KB\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>کآشوب</td>\n",
              "      <td>کآشوب  بیست سه روایت روضه هایی زندگی کنیم   ه...</td>\n",
              "      <td>داستان کوتاه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>داستان‌های برق‌آسا</td>\n",
              "      <td>داستان برق آسا  نام مجموعه داستان هایی گردآور...</td>\n",
              "      <td>داستان کوتاه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>بحثی درباره مرجعیت و روحانیت</td>\n",
              "      <td>مجموعه مقالات  بحثی مرجعیت روحانیت  شامل مقالا...</td>\n",
              "      <td>کلیات اسلام</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>قلعه‌ی حیوانات</td>\n",
              "      <td>قلعه حیوانات  جورج اورول  گروهی حیوانات اهلی ...</td>\n",
              "      <td>رمان</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>قصه ما مثل شد (۱)</td>\n",
              "      <td>قصه مثل شد  یک مجموعه کتاب  جلدی است محمد میر...</td>\n",
              "      <td>داستان کودک و نوجوانان</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          title  \\\n",
              "0                         کآشوب   \n",
              "1            داستان‌های برق‌آسا   \n",
              "2  بحثی درباره مرجعیت و روحانیت   \n",
              "3                قلعه‌ی حیوانات   \n",
              "4             قصه ما مثل شد (۱)   \n",
              "\n",
              "                                         description              categories  \n",
              "0   کآشوب  بیست سه روایت روضه هایی زندگی کنیم   ه...            داستان کوتاه  \n",
              "1   داستان برق آسا  نام مجموعه داستان هایی گردآور...            داستان کوتاه  \n",
              "2  مجموعه مقالات  بحثی مرجعیت روحانیت  شامل مقالا...             کلیات اسلام  \n",
              "3   قلعه حیوانات  جورج اورول  گروهی حیوانات اهلی ...                    رمان  \n",
              "4   قصه مثل شد  یک مجموعه کتاب  جلدی است محمد میر...  داستان کودک و نوجوانان  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "books_test.info(verbose=True,show_counts=True)\n",
        "\n",
        "books_test[\"description\"] = books_test[\"description\"].apply(clean_text)\n",
        "books_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVpS3To_sIFF"
      },
      "source": [
        "<h3>phase2: solving the problem</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">وقت حل مسئله است! ابتدایی ترین قدم، فیلتر کردن ستون توضیحات دیتا فریم تست بر اساس کتگوری میباشد. این عمل به این دلیل است که بعدتر بتوانیم کلمات مربوط به هر کتگوری را در BoW راحت تر دسته بندی کنیم. تابع زیر مسئول این کار میباشد. این تابع لیست کتگوری ها و دیتا فریم مورد بررسی را به عنوان آرگومان دریافت میکند. ابتدا یک لیست خالی برای نگه داشتن سری (منظور pandas series  است، چرا که ما فقط با ستون توضیحات کار داریم و هر ستون یک دیتا فریم در پانداز، یک سری میباشد) های فیلتر شده درست میکند. سپس در هر مرحله، با استفاده از متد loc پانداز، همه سطر های توضیحاتی که یک کتگوری دارند به هم چسبانده و در قالب پانداز سری برگردانده شده، و پس از آن به لیست ساخته شده اضافه میشود. این کار برای همه کتگوری های پاس داده شده به تابع انجام شده، سپس لیست مذکور توسط تابع ریترن میشود. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7fXohwaIxzIF"
      },
      "outputs": [],
      "source": [
        "def make_cat_filt(categories : list, books_train : pd.DataFrame) -> list:\n",
        "    category_filtered_list = list()\n",
        "    for x in categories:\n",
        "        category_filtered_list.append(\n",
        "            books_train[\"description\"].loc[books_train[\"categories\"] == x]\n",
        "            )\n",
        "    return category_filtered_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در این مرحله، باید از لیست سری های فیلتر شده مان استفاده کنیم و BoW را بسازیم. در ابتدا یک لیست خالی به نام series_to_concat میسازیم تا سری هایی که هر کدام نشان دهنده کلمات یک کتگوری هستند را در آن نگه داشته و در آخرین مرحله با هم ترکیب کنیم. در خط بعد یک حلقه فور داریم که عمل شمردن کلمات را برای هر کتگوری انجام میدهد. پیش از بررسی خط بعد، باید توضیح بدهم که هدف این است که برای هر کتگوری، یک سری بسازیم که ایندکس های آن، کلمات دیده شده در هر کتگوری و مقادیر آن، تعداد تکرار این کلمات می باشد. حال این کار را چگونه انجام میدهیم؟ در ابتدا برای هر عضو لیست فیلتر شده، از متد .str.split() استفاده میکنیم که وظیفه آن، تبدیل متن هر خانه پانداز سری به لیستی از کلمات آن متن میباشد. سپس متد .explode() همه این لیست های ایجاد شده را به هم چسبانده و در قالب یک سری بسیار بلند بر میگرداند که هر خانه آن، یک کلمه است. سپس متد .value_counts() تعداد تکرار هر کلمه را در این سری بسیار بلند شمرده و در قالب یک سری دیگر بر میگرداند که ایندکس آن، کلمات و مقادیر آن تعداد تکرار هر کلمه میباشد. در نهایت  نام سری ایجاد شده را به نام کتگوری مورد نظر تغییر داده و به لیست series_to_concat اضافه میکنیم. در خط بعد و پس از انجام مراحل بالا برای همه کتگوری ها، از متد .concat() پانداز استفاده میکنیم که این سری های ساخته شده را به یک دیتافریم تبدیل کنیم. به این متد در ابتدا لیست series_to_concat پاس داده میشود، سپس نوع اضافه شدن سری ها مشخص میشود که در اینجا اجتماع همه ایندکس ها قرار داده شده. و در مرحله بعد با قرار دادن axis= 1 به متد میفهمانیم که در نظر داریم این سری ها را به شکل ستونی به هم بچسبانیم. سپس  متد fillna(0) استفاده میشود، چرا که در دیتافریم ساخته شده، کتگوری هایی که ایندکسی را نداشته اند، برای آن مقدار nan قرار میدهد که باید این مقدار با مقدار 0 جایگزین شود (چراکه نبود ایندکس به معنی نبود آن کلمه در کتگوری مورد نظر است) در نهایت تایپ دیتافریم به عدد صحیح تغیر پیدا کرده و تابع، BoW ساخته شده را بر میگرداند</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_BoW(category_filtered_list : list, categories : list) -> pd.DataFrame:\n",
        "    series_to_concat = list()\n",
        "    for x in range(len(category_filtered_list)):\n",
        "        series_to_concat.append(category_filtered_list[x].str.split().explode().value_counts().rename(categories[x]))\n",
        "    BoW = pd.concat(series_to_concat,join=\"outer\",axis = 1).fillna(0).astype(int)\n",
        "    return BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">حال از توابع ایجاد شده استفاده کرده و BoW را تشکیل میدهیم. در ابتدا تمام کتگوری را ها با استفاده از متد unique پانداز (که کلماتی که در یک سری دیده شده اند را بر میگرداند) استخراج کرده و در قالب لیست ذخیره میکنیم. سپس category_filtered_list و BoW  را با توابع توضیح داده شده میسازیم. در نهایت نگاهی به ابتدای BoW می اندازیم. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>جامعه‌شناسی</th>\n",
              "      <th>کلیات اسلام</th>\n",
              "      <th>داستان کودک و نوجوانان</th>\n",
              "      <th>داستان کوتاه</th>\n",
              "      <th>مدیریت و کسب و کار</th>\n",
              "      <th>رمان</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>description</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>است</th>\n",
              "      <td>1375</td>\n",
              "      <td>1005</td>\n",
              "      <td>673</td>\n",
              "      <td>1131</td>\n",
              "      <td>1286</td>\n",
              "      <td>1690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>کتاب</th>\n",
              "      <td>838</td>\n",
              "      <td>598</td>\n",
              "      <td>439</td>\n",
              "      <td>477</td>\n",
              "      <td>968</td>\n",
              "      <td>748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>اجتماعی</th>\n",
              "      <td>458</td>\n",
              "      <td>50</td>\n",
              "      <td>3</td>\n",
              "      <td>40</td>\n",
              "      <td>39</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>جامعه</th>\n",
              "      <td>453</td>\n",
              "      <td>79</td>\n",
              "      <td>5</td>\n",
              "      <td>34</td>\n",
              "      <td>35</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>کند</th>\n",
              "      <td>388</td>\n",
              "      <td>156</td>\n",
              "      <td>229</td>\n",
              "      <td>341</td>\n",
              "      <td>388</td>\n",
              "      <td>727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>خود</th>\n",
              "      <td>360</td>\n",
              "      <td>213</td>\n",
              "      <td>93</td>\n",
              "      <td>268</td>\n",
              "      <td>519</td>\n",
              "      <td>473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>یک</th>\n",
              "      <td>342</td>\n",
              "      <td>143</td>\n",
              "      <td>275</td>\n",
              "      <td>485</td>\n",
              "      <td>600</td>\n",
              "      <td>806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>شود</th>\n",
              "      <td>298</td>\n",
              "      <td>156</td>\n",
              "      <td>134</td>\n",
              "      <td>226</td>\n",
              "      <td>243</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>رسانه</th>\n",
              "      <td>295</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>شناسی</th>\n",
              "      <td>292</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>دارد</th>\n",
              "      <td>272</td>\n",
              "      <td>165</td>\n",
              "      <td>101</td>\n",
              "      <td>165</td>\n",
              "      <td>209</td>\n",
              "      <td>316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>فرهنگ</th>\n",
              "      <td>246</td>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>21</td>\n",
              "      <td>41</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>فرهنگی</th>\n",
              "      <td>234</td>\n",
              "      <td>43</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>فصل</th>\n",
              "      <td>232</td>\n",
              "      <td>111</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>109</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>بررسی</th>\n",
              "      <td>230</td>\n",
              "      <td>132</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>93</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             جامعه‌شناسی  کلیات اسلام  داستان کودک و نوجوانان  داستان کوتاه  \\\n",
              "description                                                                   \n",
              "است                 1375         1005                     673          1131   \n",
              "کتاب                 838          598                     439           477   \n",
              "اجتماعی              458           50                       3            40   \n",
              "جامعه                453           79                       5            34   \n",
              "کند                  388          156                     229           341   \n",
              "خود                  360          213                      93           268   \n",
              "یک                   342          143                     275           485   \n",
              "شود                  298          156                     134           226   \n",
              "رسانه                295            5                       0             4   \n",
              "شناسی                292           45                       0             5   \n",
              "دارد                 272          165                     101           165   \n",
              "فرهنگ                246           35                       3            21   \n",
              "فرهنگی               234           43                       3            14   \n",
              "فصل                  232          111                       0             7   \n",
              "بررسی                230          132                       1             6   \n",
              "\n",
              "             مدیریت و کسب و کار  رمان  \n",
              "description                            \n",
              "است                        1286  1690  \n",
              "کتاب                        968   748  \n",
              "اجتماعی                      39    54  \n",
              "جامعه                        35    51  \n",
              "کند                         388   727  \n",
              "خود                         519   473  \n",
              "یک                          600   806  \n",
              "شود                         243   500  \n",
              "رسانه                        12     2  \n",
              "شناسی                        31     6  \n",
              "دارد                        209   316  \n",
              "فرهنگ                        41    21  \n",
              "فرهنگی                       14    10  \n",
              "فصل                         109    34  \n",
              "بررسی                        93    14  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories = list(books_train['categories'].unique())\n",
        "category_filtered_list = make_cat_filt(categories, books_train)\n",
        "BoW = make_BoW(category_filtered_list, categories)\n",
        "BoW.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">همانطور که مشاهده میکنید، حروف اضافه و علائم و اعداد از بین رفته اند، اما هنوز برخی کلمات مانند «است» در دیتافریم وجود دارند که به نظر میرسد خیلی به درد بخور نیستند! برای آنها در بخش امتیازی تصمیم میگیریم.</p>\n",
        "<hr></hr>\n",
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">تا به اینجا بنظر پیشرفت خیلی خوبی داشتیم! حال نوبت آن رسیده است که مقدمات محاسبه احتمال را فراهم کنیم. در ابتدا باید احتمال هر کتگوری را توسط تابع زیر محاسبه کنیم. این تابع category_filtered_list و books_train و categories را به عنوان آرگومان دریافت کرده و یک لیست خالی برای نگهداری تعداد کتاب های هر کتگوری میسازد. سپس  تعداد کتاب های هر کتگوری را به صورت جدا به وسیله حرکت روی category_filtered_list پیدا کرده و به لیست اضافه میکند (شمردن تعداد توضیحات برابر با شمردن تعداد کتاب ها است). پس از آن، تعداد کل کتاب ها را شمرده در مرحله بعد لیست نگهداری تعداد را به پانداز سری تبدیل میکند که بتوانیم روی آن عملیات جبری انجام دهیم. سپس سری مورد نظر را به تعداد کل کتاب های تقسیم میکند (هر خانه سری به این مقدار تقسیم میشود) و ایندکس های آن را کتگوری هایمان قرار داده و آن را بر میگرداند.   </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_prior(category_filtered_list : list, books_train : pd.DataFrame , categories : list) -> pd.Series:\n",
        "    categories_books_count = []\n",
        "    for x in category_filtered_list:\n",
        "        categories_books_count.append(len(x))\n",
        "    sum_of_books = len(books_train.index)\n",
        "    categories_books_count = pd.Series(categories_books_count)\n",
        "    prior = categories_books_count / sum_of_books\n",
        "    prior.index = categories\n",
        "    return prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">احتمال پیشینمان را با تابع توضیح داده شده محاسبه و سپس مشاهده میکنیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "جامعه‌شناسی               0.166667\n",
              "کلیات اسلام               0.166667\n",
              "داستان کودک و نوجوانان    0.166667\n",
              "داستان کوتاه              0.166667\n",
              "مدیریت و کسب و کار        0.166667\n",
              "رمان                      0.166667\n",
              "dtype: float64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prior = calc_prior(category_filtered_list, books_train, categories)\n",
        "prior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در اینجا بر حسب اتفاق، همه کتگوری ها احتمال برابر داشتند!</p>\n",
        "<hr></hr>\n",
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">حال میخواهیم  کلمات موجود در بخش توضیحات دیتا فریم تست را در قالب یک سری که در هر خانه آن لیست کلمات توضیحات هر کتاب قرار دارد داشته باشیم. دوباره از متد str.split استفاده میکنیم و برای اینکه یک کلمه تکراری نباشد، از متد unique استفاده میکنیم.در لامبدا اکسپرشن نوشته شده، x هر سطر از سری است که یک لیست میباشد، و به این معنی است که \n",
        "unique مقدار هر سطر را به عنوان آرگومان دریافت کرده و مقدار بازگشتی اش را در همان سطر ذخیره میکند.  </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_books_test_desc(books_test : pd.DataFrame) -> pd.Series:\n",
        "    books_test_desc = books_test[\"description\"].str.split()\n",
        "    books_test_desc = books_test_desc.apply(lambda x: pd.unique(x))\n",
        "    return books_test_desc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">متغیر books_test_desc را با تابع بالا ساخته و نگاهی به ابتدای آن می اندازیم.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    [کآشوب, بیست, سه, روایت, روضه, هایی, زندگی, کن...\n",
              "1    [داستان, برق, آسا, نام, مجموعه, هایی, گردآوری,...\n",
              "2    [مجموعه, مقالات, بحثی, مرجعیت, روحانیت, شامل, ...\n",
              "3    [قلعه, حیوانات, جورج, اورول, گروهی, اهلی, است,...\n",
              "4    [قصه, مثل, شد, یک, مجموعه, کتاب, جلدی, است, مح...\n",
              "Name: description, dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "books_test_desc = make_books_test_desc(books_test)\n",
        "books_test_desc.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>phase2-1: without additive smoothing</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">امیدوارم تا به اینجای کار از برنامه لذت برده باشید! در این مرحله قصد داریم دیتا فریم BoW را تبدیل به دیتا فریمی کنیم که رو به روی هر کلمه، احتمال آن کلمه به شرط کتگوری آن کلمه آمده باشد (بدون ادیتیو اسموتینگ). این عمل در تسریع محاسبات کمک شایانی میکند. احتمال هر کلمه به شرط کتگوری را هم طبق فرمول به شکل تقسیم تعداد تکرار آن کلمه در کتگوری بر تعداد کل کلمات کتگوری تعریف میکنیم. در اینجا با استفاده از قابلیت های پانداز و قرار دادن axis = 0 در هنگام جمع ، کل مقادیر هر ستون با هم جمع شده و در نهایت یک سری با ایندکس کتگوری ها و مقدار تعداد کلمات کتگوری به ما بر میگرداند که سپس با  تقسیم همه سطر های دیتا فریم بر این سطر تشکیل شده (این بار اکسیس را یک قرار میدهیم تا این عمل انجام شود) دیتا فریم مد نظر تشکیل میشود و آن را بر میگردانیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_prob_df(df : pd.DataFrame) -> pd.DataFrame:\n",
        "    final_df = df.copy()\n",
        "    observ_cnt = df.sum(axis=0)\n",
        "    final_df = final_df.div(observ_cnt,axis=1)\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">پیش از آنکه مرحله آخر این بخش را پیاده سازی کنیم، لازم است به پرسش دوم پاسخ بدهیم (نگران نباشید جلوتر به پرسش اول هم میرسیم!). احتمال هر کلمه به شرط کتگوری مد نظر، مقداری کوچک میشود. حال فرض کنید به تعداد کلمات یک پاراگراف، از این اعداد کوچک در هم ضرب کنیم. احتمال آنقدر کم میشود که ممکن است در محاسبات کامپیوتری دچار خطا بشویم. با لگاریتم گرفتن از این مقادیر و جمع آنها، با اعدادی از اردر های نه چندان بزرگ و منفی سر و کار خواهیم داشت بعلاوه اینکه بین آنها هم جمع قرار میگیرد، بنابراین نیاری نیست نگران خیلی بزرگ شدن مقدار نهایی هم باشیم و مشکل خطای محاسبات اینگونه رفع میشود.</p>\n",
        "<hr></hr>\n",
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">حال با توضیحات داده شده برویم سراغ اصل  مطلب! تابع زیر لیست کلمات توضیحات کتاب بعلاوه دیتا فریم احتمالات و سری احتمال پیشین را به عنوان آرگومان میپذیرد. سپس در ابتدا، بین کلمات کتاب و کلمات BoW با کمک متد intersection اشتراک میگیرد (این کار به این دلیل است که اگر کلمه ای در BoW نباشد، به ناچار در محاسبات، احتمال هر کتگوری را برای آن یکسان در نظر میگیریم. این هم ارز است با اینکه این کلمه را به کلی در نظر نگیریم و همان ابتدا از کلمات مورد محاسبه بیرون بریزیم). در مرحله بعد، زیر دیتافریمی از دیتافریم احتمالات فقط شامل کلمات متن میسازیم تا در محاسبات آینده صرفه جویی شود. سپس متد log  از کتابخانه نامپای را روی دیتا فریم اعمال میکنیم (آرگومان های آن برای این است که لگاریتم احتمالات صفر را برابر با صفر قرار دهد و ارور دریافت نکنیم. برابر با صفر قرار دادن این کلمات  هم ارز با ندید گرفتن آنها در محاسبات مربوط به بیز ساده است). در مرحله بعد، با جمع مقادیر هر ستون، به مقدار مورد نظر بیز ساده برای هر کتگوری میرسیم. سپس، سری به دست آمده که شامل کتگوری ها به عنوان کلید و احتمال متن به شرط کتگوری به عنوان مقدار میباشد را در سری احتمال پیشین ضرب میکنیم. مرحله آخر هم که تقسیم همه مقادیر نهایی بر احتمال متن است را نادیده میگیریم چرا که در مقایسه این احتمالات تاثیری ندارد. در نهایت ایندکسی که بیشترین مقدار را در سری probability دارد بر میگردانیم. این ایندکس در واقع کتگوری تخمین زده شده برای متن مورد نظر است. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def cal_prob(text : list, prob_bow : pd.DataFrame, prior : pd.Series):\n",
        "  existing_words = prob_bow.index.intersection(text)\n",
        "  words_df = prob_bow.loc[existing_words]\n",
        "  log_words = pd.DataFrame(np.log(\n",
        "    words_df\n",
        "    ,out= np.zeros_like(words_df)\n",
        "    ,where=(words_df != 0)))\n",
        "  likelihood = log_words.sum(axis = 0)\n",
        "  probability = likelihood * prior\n",
        "  return probability.idxmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">توابع توضیح داده شده را بر روی books_test_desc که شامل کلمات هر کتاب به شکل لیست است با کمک لامبدا اکسپرشن اپلای میکنیم. لامبدا اکسپرشن هم به این معنی است که هر سطر این دیتا فریم را به عنوان آرگومان اول که لیست کلمات است به تایع cal_prob بده و بقیه آرگومان ها را هم  prob_bow , prior قرار بده. سپس نتایج به دست آمده را با کتگوری های واقعی مقایسه میکنیم. نتیجه را ببینیم:</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    441\n",
              "True       9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob_bow = make_prob_df(BoW)\n",
        "categories_list_noadt = books_test_desc.apply(lambda x: cal_prob(x,prob_bow,prior))\n",
        "(categories_list_noadt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">مشاهده میکنیم که یک فاجعه ملی رخ داد! حال وقت آن است که به پرسش اول پاسخ دهیم. ندید گرفتن هر کلمه با احتمال صفر در کتگوری به این معنا است که در فرمول بیز ساده، احتمال آن را یک در نظر بگیریم، به عبارت دیگر ما با این کار احتمال این کلمه را از کمترین مقدار ممکن به بیشترین مقدار ممکن میبریم! پیامد این عمل آن است که هر کتگوری که صفر بیشتری داشته باشد هر دفعه برنده میشود. چاره این اشتباه فرمول ادیتیو اسموتینگ است که جلوتر به آن میپردازیم. ولی اگر بخواهیم بدون این فرمول کمی نتایج را بهتر کنیم، لازم است ارزش یک را تا جای ممکن در محاسبات خود کاهش دهیم. این کار چگونه ممکن است؟ درست حدس زدید، با اسکیل کردن مقادیر احتمال ها. حال بیاید بدون ورود به ادیتیو اسموتینگ، با اسکیل 2000، یک بار دیگر نتیجه را بررسی کنیم:  </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    235\n",
              "True     215\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categories_list_noadt_2 = books_test_desc.apply(lambda x: cal_prob(x,prob_bow * 2000,prior))\n",
        "(categories_list_noadt_2 == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">خیلی بهتر شد، اما هنوز هم کافی نیست. برای باز هم بهتر شدن، بروید سراغ بخش بعد!</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>phase 2-2: with additive smoothing</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">ادیتیو سموتینگ در واقع به همه احتمال های صفر، بر اساس مجموع مقادیر دسته بندی و تعداد مقادیر یکتا، یک احتمال خیلی کوچک میدهد. تابع زیر این عمل را بر اساس فرمول ادیتیو اسموتینگ روی دیتا فریم مورد نظر اعمال میکند. در ابتدا دیتا فریم مورد نظر را به عنوان آرگومان دریافت میکند. سپس مقادیر یکتای هر ستون را پیدا کرده و در قالب یک سری نگه میدارد (در مورد لامبدا اکسپرشن و یونیک قبلا توضیح داده شد، فقط قرار دادن axis = 1 موجب میشود حرکت اپلای به جای سطری، ستونی شود). سپس تعداد همه کلمات یک کتگوری را به دست می آورد و در قالب یک سری با کلید کتگوری و مقدار تعداد کلمات نگه میدارد. سپس adt_value یا همان آلفا در فرمول اصلی محاسبه میشود. علت اینکه این مقدار ثابت قرار داده نشد این بود که در صورت ثابت بودن، برای کتگوری با تعداد کلمات کمتر، وزن بیشتری پیدا میکرد، فلذا آلفا متناسب با مجموع تعداد کلمات قرار داده شد (ضریب تناسب با آزمون و خطا به دست آمده) . در مرحله آخر روی دیتافریم پاس داده شده فرمول ادیتیو اسموتینگ پیاده میشود (با استفاده از قابلیت های پانداز، همه خانه های دیتا فریم با یک مقدار ثابت جمع میشود، سپس همه سطر های دیتا فریم بر یک سطر تقسیم میشود که در واقع حاصل جمع سری مجموع کلمات و سری مقدار های یکتا ضرب در آلفا میباشد). در نهایت دیتا فریم اسموت شده را برگردانده میشود.  </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "v4aMGlZdaif9"
      },
      "outputs": [],
      "source": [
        "def make_adt_smooth(df : pd.DataFrame):\n",
        "  final_df = df.copy()\n",
        "  unique_vals = df.apply(lambda x : len(pd.unique(x)), axis=0)\n",
        "  observ_cnt = df.sum(axis=0)\n",
        "  adt_value = observ_cnt * 0.0001\n",
        "  final_df = (final_df + adt_value).div(observ_cnt + (adt_value * unique_vals),axis=1)\n",
        "  return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">محاسبات را یک بار دیگر با ادیتیو اسموتینگ انجام داده و نتیجه را مشاهده میکنیم:</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "M64M1Hjp5blZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     364\n",
              "False     86\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_BoW = make_adt_smooth(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_adt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">نتیجه بسیار خوب است! تقریبا 80 درصد ورودی ها به درستی تشخیص داده شدند و با تقریب خوبی میتوان گفت به خواسته مطلوبمان رسیدیم.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>bonus content</h2>\n",
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">تبریک بابت تمام کردن بخش معمولی پروژه! وقت آن است سراغ محتوای امتیازی برویم.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>bonus part1: lemetize  and stem</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>bonus part1-1: lemetize</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">کتابخانه هضم دو تابع مفید برای پری پراسس به نام stem و lemetize دارد که استم وظیفه حذف کردن پیشوند و پسوند اضافه کلمه بدون توجه به معنی کلمه و تابع لمتایز وظیفه دارد کلمه را به بن ماضی و مضارع خود تبدیل کند. ببینم هر تابع چقدر میتوان مفید یا مضر باشد.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">تابع lem_text عمل لمتایز را روی کلمه مورد نظر انجام داده و از آنجایی که کتابخانه هضم بن ماضی و مضارع را با # از هم جدا میکند، باید این علامت را با اسپیس جای گذاری کرده و برگرداند. تابع clean_text_lem  نیز روی هر کلمه این تابع را اجرا میکند. بقیه اجزا مانند گذشته است.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "lem_obj = hz.Lemmatizer()\n",
        "def lem_text(text : str) -> list:\n",
        "  return lem_obj.lemmatize(text).replace('#',' ')\n",
        "\n",
        "def clean_text_lem(text : str) -> str:\n",
        "  text = norm_obj.normalize(text).replace(\"‌\",\" \")\n",
        "  tokenized_text_list = token_obj.tokenize(text)\n",
        "  semi_final_text = \" \".join([lem_text(x) for x in tokenized_text_list if x not in words_to_delete])\n",
        "  final_text = \"\".join([x for x in semi_final_text if x not in spec_chars])\n",
        "  return final_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">همه کار هایی که پیشتر انجام داده بودیم را دوباره با تابع پری پراسس جدیدمان انجام میدهیم.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "books_test = pd.read_csv(\"books_test.csv\")\n",
        "books_train = pd.read_csv(\"books_train.csv\")\n",
        "\n",
        "books_train[\"description\"] = books_train[\"description\"].apply(clean_text_lem)\n",
        "books_test[\"description\"] = books_test[\"description\"].apply(clean_text_lem)\n",
        "\n",
        "categories = list(books_train['categories'].unique())\n",
        "category_filtered_list = make_cat_filt(categories, books_train)\n",
        "BoW = make_BoW(category_filtered_list, categories)\n",
        "\n",
        "books_test_desc = make_books_test_desc(books_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>without additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">حال دوباره تابع بدون ادیتیو اسموتینگ خود را روی این BoW جدید امتحان میکنیم</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    441\n",
              "True       9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob_bow = make_prob_df(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_noadt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">تغییر خاصی در این بخش ایجاد نشد</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>with additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">و حالا ادیتیو اسموتینگ</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     359\n",
              "False     91\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_BoW = make_adt_smooth(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_adt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">علت کمتر شدن این مقدار، این است که کلماتی که یکسان نیستند مانند اندیشمند و اندیشه و هم اندیش، هر سه تبدیل به بن خود یعنی اندیش میشوند و این موجب اختلال در محاسبه احتمال میشود. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>bonus part1-2: stem</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در این بخش میپردازیم به تابع استم. تابع stem_text وظیفه استم کردن استرینگ ورودی به آن را دارد. برای استم شدن هر کلمه، این  تابع را داخل لیست کلمات توکنایز شده بر هر کلمه اجرا میکنیم. بقیه قسمت ها مانند قبل میباشد.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "stem_obj = hz.Stemmer()\n",
        "def stem_text(text : str) -> str:\n",
        "  text = stem_obj.stem(text)\n",
        "  return text\n",
        "\n",
        "def clean_text_stem(text : str) -> str:\n",
        "  text = norm_obj.normalize(text).replace(\"‌\",\" \")\n",
        "  tokenized_text_list = token_obj.tokenize(text)\n",
        "  semi_final_text = \" \".join([stem_text(x) for x in tokenized_text_list if x not in words_to_delete])\n",
        "  final_text = \"\".join([x for x in semi_final_text if x not in spec_chars])\n",
        "  return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "books_test = pd.read_csv(\"books_test.csv\")\n",
        "books_train = pd.read_csv(\"books_train.csv\")\n",
        "\n",
        "books_train[\"description\"] = books_train[\"description\"].apply(clean_text_lem)\n",
        "books_test[\"description\"] = books_test[\"description\"].apply(clean_text_lem)\n",
        "\n",
        "categories = list(books_train['categories'].unique())\n",
        "category_filtered_list = make_cat_filt(categories, books_train)\n",
        "BoW = make_BoW(category_filtered_list, categories)\n",
        "\n",
        "books_test_desc = make_books_test_desc(books_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>without additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">ببینیم چقدر وضع بهتر یا بدتر میشود</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    441\n",
              "True       9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob_bow = make_prob_df(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_noadt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>with additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     359\n",
              "False     91\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_BoW = make_adt_smooth(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_adt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">همانطور که مشاهده کردید، نتایج این بخش هیچ تفاوتی با بخش لمتایز نداشتند. چرا که این دو در واقع کاری شبیه به هم را انجام میدهند و تفاوت جزئی آنها موجب نمیشود تغییر چندانی در خروجی به دست بیاید</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>bonus part2: stop words</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">بخش مورد بحث بعدی استاپ وردز است که از طریق لیست آماده کتابخانه هضم و چک کردن آن در کنار لیست خودمان آن را در پری پراسس وارد میکنیم. استاپ وردز کلمات پرتکراری هستند که در جمله ها بسیار دیده میشوند ولی بودن یا نبودن آنها بار معنایی به کلمات BoW اصافه نمیکند، مانند است، بود شد و... . بقیه قسمت ها مانند قسمتهای قبل میباشند.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = hz.stopwords_list()\n",
        "\n",
        "def clean_text_sto(text : str) -> str:\n",
        "  text = norm_obj.normalize(text).replace(\"‌\",\" \")\n",
        "  tokenized_text_list = token_obj.tokenize(text)\n",
        "  semi_final_text = \" \".join([x for x in tokenized_text_list if x not in words_to_delete and x not in stop_words])\n",
        "  final_text = \"\".join([x for x in semi_final_text if x not in spec_chars])\n",
        "  return final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "books_test = pd.read_csv(\"books_test.csv\")\n",
        "books_train = pd.read_csv(\"books_train.csv\")\n",
        "\n",
        "books_train[\"description\"] = books_train[\"description\"].apply(clean_text_sto)\n",
        "books_test[\"description\"] = books_test[\"description\"].apply(clean_text_sto)\n",
        "\n",
        "categories = list(books_train['categories'].unique())\n",
        "category_filtered_list = make_cat_filt(categories, books_train)\n",
        "BoW = make_BoW(category_filtered_list, categories)\n",
        "\n",
        "books_test_desc = make_books_test_desc(books_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>without additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">ببینیم آیا استاپ وردز کمکی به دقت ما میکند؟</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    441\n",
              "True       9\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prob_bow = make_prob_df(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_noadt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">در این بخش چیزی اضافه نمیشود</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4>with additive smoothing</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True     369\n",
              "False     81\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_BoW = make_adt_smooth(BoW)\n",
        "categories_list_adt = books_test_desc.apply(lambda x: cal_prob(x,smoothed_BoW,prior))\n",
        "(categories_list_adt == books_test[\"categories\"]).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">و بالاخره با ادیتیو اسموتینگ، به دقت کد ما اضافه شد. یکی از عللی که کار های بخش امتیازی کمک چشمگیری در خروجی نکردند این بود که فایل شخصی words to delete تا حد خوبی برخی ازین موارد را پوشش میداد. از طرفی در مورد بخش بدون ادیتیو اسموتینگ خطا آنقدر بالا هست که تغییرات کوچک در پری پراسس تاثیر چندانی روی این بخش ها نداشته باشد. در کل بهترین نتیجه همانطور که شخصا انتظار داشتم، با استاپ وردز به دست آمد.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p dir=\"rtl\" lang=\"fa\" align=\"center\">پاسخ به سوال نهایی امتیازی: توابع لمتایز و استم به ذاته توابع خوبی هستند، اما به دلایلی از جمله قاطی شدن بن های کلمات و اشتباه در استم کردن (به طور مثال رضوان را به رضو تبدیل میکند) بهتر است به تنهایی استفاده شوند و در ترکیب با یک لیست استاپ وردز استفاده نشوند، چرا که استاپ وردز علاوه بر افعال و اسامی، به تنهایی برخی کلمات و پیشوند ها و پسوند ها را حذف میکند و به این ترتیب ممکن است این توابع با انجام عمل بی مورد روی دیتا، درستی آن را کمتر کنند. نوع کاکرد آنها نیز پیشتر توضیح داده شد. در نهایت در صورتی که بخواهیم بین استاپ وردز و این توابع هضم یکی را انتخاب کنیم، بهتر است استاپ وردز انتخاب شود چرا که دقیق است و احتمال اشتباه در آن خیلی کم میباشد و از طرفی اورهد کمتری نسبت به این توابع کتابخانه هضم دارد.</p>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
